import nltk
import random
#from nltk.corpus import movie_reviews
from nltk.classify.scikitlearn import SklearnClassifier
import pickle
from sklearn.naive_bayes import MultinomialNB, BernoulliNB
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.svm import SVC, LinearSVC, NuSVC
from nltk.classify import ClassifierI
from statistics import mode
from nltk.tokenize import word_tokenize
import pandas as pd
from nltk.corpus import stopwords
import math

class VoteClassifier(ClassifierI):
    def __init__(self, *classifiers):
        self._classifiers = classifiers

    def classify(self, features):
        votes = []
        for c in self._classifiers:
            v = c.classify(features)
            votes.append(v)
        return mode(votes)

    def confidence(self, features):
        votes = []
        for c in self._classifiers:
            v = c.classify(features)
            votes.append(v)

        choice_votes = votes.count(mode(votes))
        conf = choice_votes / len(votes)
        return conf
    
#documents receives a dataframe. the name of the column with the text and the column with the binary category
#names of columns have to be strings
#returns documents: a list of tuples

class text_class:
    def __init__(self,dataframe,text,category):

        true_docs = dataframe[dataframe[category]==1]
        false_docs = dataframe[dataframe[category]==0]
        self.result = "Documents have been saved. Number of ture {} number of false {}".format(len(true_docs),len(false_docs)) 
        if len(true_docs)>len(false_docs):
            true_docs = true_docs.sample(n = len(false_docs))
        else:
            false_docs = false_docs.sample(n = len(true_docs))

        data = true_docs.append(false_docs)
        documents = []
        for ind, row in dataframe.iterrows():
            words = word_tokenize(row[text].lower())
            Lista = [words,row[category]]
            documents.append(Lista)
            
        self.result_2 = "Documents have been saved. Number of ture {} number of false {}".format(len(true_docs),len(false_docs)) 
        self.docs = documents
        print(self.result,self.result_2)

    #creates list of tuples with the documents
    def documents(self):
        save_documents = open('pickled_algos/documents.pickle',"wb")
        pickle.dump(self.docs, save_documents)
        save_documents.close()
        
        return self.docs
    


#feed by a list of tuples
class docs_class:
    #returns a list of the documents where the category is true or takes value 1
    def __init__(self, list_docs):
        self.docs = list_docs
        print(type(list_docs))
        
        
    def words(self, method="tfidf",language='spanish',stop_words = True,extra_stop = [],remove_single = True):
        stop_words = set(stopwords.words(language))
        extra = []
        list_words = []
        for i in self.docs:
                list_words.append(i[0])

        list_words_2 = []        
        if stop_words==True:
            for l in list_words:
                ind_list = []
                for w in l:
                    if w not in stop_words:
                        ind_list.append(w)
                    else:
                        pass
                list_words_2.append(ind_list)
        else:
            list_words_2 = list_words
        
        list_words_3 = []
        if len(extra_stop)>1:
            for l in list_words_2:
                ind_list = []
                for w in l:
                    if w not in extra_stop:
                        ind_list.append(w)
                    else:
                        pass
                list_words_3.append(ind_list)
        else:
            list_words_3 = list_words_2
                   
        list_words_4 = []
        if remove_single==True:
            for l in list_words_3:
                ind_list = []
                for w in l:
                    if len(w)>1:
                        ind_list.append(w)
                    else:
                        pass
                list_words_4.append(ind_list)
        else:
            list_words_4 = list_words_3
                   

        list_words = list_words_4
            
        dict_words = dict()
        for l in list_words:
            for w in l:
                if w in dict_words:
                    dict_words[w] = dict_words[w] + 1
                else:
                    dict_words[w] = 1        
        #method:
            #freq: term frequency in the documents
            #tfidf: term frequency inverse document frecuency
        if method == 'freq':
            dict_freq = {k: v for k,v in sorted(dict_words.items(),key=lambda item:item[1], reverse = True)} 
            list_freq =  sorted(dict_words.items(),key=lambda x: x[1], reverse = True)        
            return list_freq, dict_freq 

        elif method == 'tfidf':
            dict_freq = {k: v for k,v in sorted(dict_words.items(),key=lambda item:item[1])} 
            dict_number = dict()
            words_list = [] 
            for k in dict_freq.keys():
                words_list.append(k)

            for word in words_list:
                for l in list_words:
                    if word in l:
                        if word in dict_number:
                            dict_number[word] = dict_words[word] + 1
                        else:
                            dict_number[word] = 1
                    else:
                        pass
            #ahora log*#docs/1+#docs using word
            N = len(list_words)
            dict_inverse = dict()
            for w in words_list:
                if w in dict_inverse:
                    pass
                else:
                    dict_inverse[w] = math.log(N) - math.log(1+dict_number[w])
            
            dict_tfidf = dict()
            for w in words_list:
                if w in dict_tfidf:
                    pass
                else:
                    dict_tfidf[w] = dict_freq[w]*dict_inverse[w]
                
            list_tfidf =  sorted(dict_tfidf.items(),key=lambda x: x[1], reverse = True)           
            dict_tfidf =  {k: v for k,v in sorted(dict_tfidf.items(),key=lambda item:item[1], reverse = True)} 
            return list_tfidf,dict_tfidf
    

class text_features:
    def __init__(self, doc_list,word_dict):
        self.docs = doc_list
        self.words = list(word_dict.keys())

    def word_features(self,number):
        word_features = self.words[:number]
        self.word_features = word_features
        save_word_features = open("pickled_algos/word_features.pickle","wb")
        pickle.dump(word_features, save_word_features)
        save_word_features.close()
        return word_features

    def train_test_set(self,training=0.5):
        docs_training = self.docs 
        random.shuffle(docs_training)
        def find_features(text):
            words = set(text)
            feats= {}    
            for w in self.words:
                feats[w] = (w in words)
            return feats

        featuresets = [(find_features(rev), category) for (rev, category) in docs_training]
        N = len(featuresets)
        len_training = int(N*training)
        training_set = featuresets[:len_training]
        testing_set = featuresets[len_training:]
        return training_set,testing_set
    
def find_feature(text,word_dict):   
    word_list = list(word_dict.keys()) 
    words = set(text) #list to a set we get one iteration of any unique element. All the words
    feats = {}
    for w in word_list:
        feats[w] = (w in words) 
    return feats

def logistic_training(train,test):
    LogisticRegression_classifier = SklearnClassifier(LogisticRegression())
    LogisticRegression_classifier.train(train)
    statement = "LogisticRegression_classifier accuracy percent: {}".format((nltk.classify.accuracy(LogisticRegression_classifier, test))*100)  

    save_classifier = open("pickled_algos/LogisticRegression_classifier.pickle","wb")
    pickle.dump(LogisticRegression_classifier, save_classifier)
    save_classifier.close()
    return statement


def classic_classifier(train,test):
    classifier = nltk.NaiveBayesClassifier.train(train)
    statement = "Original Naive Bayes Algo accuracy percent: {}".format((nltk.classify.accuracy(classifier, test))*100)

    save_classifier = open("pickled_algos/classifier.pickle","wb")
    pickle.dump(classifier, save_classifier)
    save_classifier.close()
    return statement

def log_classification(text,dict_tfidf):
    open_file = open("pickled_algos/LogisticRegression_classifier.pickle", "rb")
    LogisticRegression_classifier = pickle.load(open_file)
    open_file.close()

